---
title: "analysis"
author: "Amber Thomas"
date: "1/2/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This file exists to download data and process it for use in our web project. 

## Loading Packages

```{r}
library(tidyverse)
library(googledrive)
library(here)
```

## Downloading Data

All of the data exists in Google Drive as several separate files. I'll need to pull them all down, append the category (from the file name), combine them all into one file, and then remove any that haven't yet been quality checked. 

```{r}
categories <- c("edutainment", "feminism", "inspiration", "latina", "style", "travel")
# TODO: Add "cultural_icon" back in
```

```{r}
colnames <- c("id", "GUID_x", "text", "handle", "profane_words", "profanity_flag", "promo_flag", "categories", "primary_category", "parent_categories", "zeno_notes", "zeno_remove", "zeno_corrected", "star_tweet", "qa")

downloadData <- function(category){
  name <- paste0(category, "_for_qa")
  path <- paste0(category, ".csv")
  
  googledrive::drive_download(name, path = here::here("large_files", path), type = "csv", overwrite = TRUE)
  
  data <- read.csv(here::here("large_files", path), stringsAsFactors = FALSE, header = TRUE, col.names = colnames, na.strings = c("", " ")) %>%
    select(-c(id, GUID_x, profane_words, zeno_notes)) %>%
    # remove if zeno_remove is not blank
    filter(is.na(zeno_remove),
      # remove if not QA'd yet
            !is.na(qa),
      # remove if profanity
            #profanity_flag == FALSE,
      # remove if promo
            promo_flag == FALSE) %>%
    mutate(cleanParent = gsub("\\[|\\'|]", "", parent_categories)) %>% 
    mutate(category = ifelse(is.na(zeno_corrected), cleanParent, zeno_corrected)) %>% 
    select(c(text, handle, category, star_tweet))
}
```

```{r}
cleanCultural <- read.csv(here::here("large_files", "cultural_icon.csv"), stringsAsFactors = FALSE, header = TRUE, col.names = colnames, na.strings = c("", " ")) %>%
  select(-c(id, GUID_x, profane_words, zeno_notes)) %>%
    # remove if zeno_remove is not blank
    filter(is.na(zeno_remove),
      # remove if not QA'd yet
            !is.na(qa),
      # remove if profanity
            #profanity_flag == FALSE,
      # remove if promo
            promo_flag == FALSE) %>%
    mutate(cleanParent = gsub("\\[|\\'|]", "", parent_categories)) %>% 
    mutate(category = ifelse(is.na(zeno_corrected), cleanParent, zeno_corrected)) %>% 
    select(c(text, handle, category, star_tweet))
```


```{r message = false}
allData <- map_dfr(categories, downloadData) %>% 
  bind_rows(cleanCultural) %>% 
  separate_rows(category, sep = ",") %>% 
  mutate(category = trimws(category))
```


Now, we need to look at just the starred tweets. These will be exported as their own independent file and used in the `curate` section. 

```{r}
fashionCat <- c("style", "stye", "style - cosplay", "style - costume", "fashion", "cosplay", "halloween", "cultural style")
feminismCat <- c("feminism", "femnisim", "feminist", "feminsim", "role model")
eduCat <- c("edutainment only", "edutainment")
iconCat <- c("lgbtq", "lgbt", "games", "pop culture", "pop culture", "cultural icon", "nostalgia", "waldo", "game", "rockapella", "search", "like carmen", "nosalgia", "icon", "theme song", "pop culture game", "pop", "nostaligia", "netflix", "nostalia", "pop sugar")

allDataCleaned <- allData %>% 
  mutate(category = tolower(category)) %>% 
  mutate(category = case_when(
    category %in% fashionCat ~ "fashion",
    category %in% feminismCat ~ "role-model",
    category == "role model" ~ "role-model", 
    category %in% eduCat ~ "edutainment",
    category %in% iconCat ~ "cultural-icon",
    grepl("pop culture", category) ~ "cultural-icon",
    grepl("pop - culture", category) ~ "cultural-icon",
    category == "latina" ~ "role-model",
    category == "inspiration" ~ "role-model",
    TRUE ~ category
  ))

starred <- allDataCleaned %>% 
  filter(!is.na(star_tweet))

write.csv(starred, "../../src/assets/data/curate.csv", row.names = FALSE, na = "")
```

Now, we need to randomize all of the tweets, and split them into separate files, each containing 1000 tweets. 

```{r}
# Make 10 random starred tweets "special"
randomStar <- starred[sample(nrow(starred)), ]
special <- randomStar %>% 
  filter(row_number() <= 10)

## Write special to file

write.csv(special, "../../src/assets/data/explore-special.csv", row.names = FALSE, na = "")


## Combine unspecial starred with non-starred, randomize, and split
unspecial <- randomStar %>% 
  filter(row_number() > 10)

nonstarred <- allDataCleaned %>% 
  filter(is.na(star_tweet))

allNonSpecial <- rbind(unspecial, nonstarred)

randomized <- allNonSpecial[sample(nrow(allNonSpecial)), ]
```

```{r}
number <- 1000

startRows <- seq(1, nrow(randomized), by = number) 
endRows <- seq(number, nrow(randomized), by = number)
endRowsAll <- append(endRows, nrow(randomized))

splitFiles <- function(startRow, endRow){
  
  data <- randomized[startRow:endRow, ]
  
  fileName <- paste0("explore-", ceiling(endRow / number), ".csv")
  
  write.csv(data, paste0("../../src/assets/data/", fileName), row.names = FALSE, na = "")
  
}

map2_dfr(startRows, endRowsAll, splitFiles)
```



